import cv2
import mediapipe as mp
import pickle
import numpy as np
import math

# ==========================================
# âš™ï¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª (Settings)
# ==========================================
MODEL_PATH = 'hand_gesture_model.pkl'
CLASPED_DISTANCE_THRESHOLD = 120  # Ø§Ù„Ù…Ø³Ø§ÙØ© Ø¨Ø§Ù„Ø¨ÙƒØ³Ù„ Ù„Ø§Ø¹ØªØ¨Ø§Ø± Ø§Ù„ÙŠØ¯ Ù…Ø´Ø¨ÙƒØ©
MOVEMENT_THRESHOLD = 5.0          # Ù„ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø§Ù‡ØªØ²Ø§Ø²Ø§Øª Ø§Ù„Ø¨Ø³ÙŠØ·Ø© Ø¬Ø¯Ø§Ù‹

# ==========================================
# 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
# ==========================================
try:
    with open(MODEL_PATH, 'rb') as f:
        model = pickle.load(f)
    print(f"âœ… Model loaded: {MODEL_PATH}")
except Exception as e:
    print(f"âŒ Error loading model: {e}")
    exit()

# Ø¥Ø¹Ø¯Ø§Ø¯ MediaPipe
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
hands = mp_hands.Hands(
    static_image_mode=False,
    max_num_hands=2,
    min_detection_confidence=0.5, # Ù‚Ù„Ù„Ù†Ø§ Ø§Ù„Ø±Ù‚Ù… Ù‚Ù„ÙŠÙ„Ø§Ù‹ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙƒØ´Ù ÙÙŠ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
    min_tracking_confidence=0.5
)

def analyze_video(video_path):
    cap = cv2.VideoCapture(video_path)
    
    if not cap.isOpened():
        print(f"âŒ Error: Could not open video file {video_path}")
        return

    # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    print(f"ğŸï¸ Processing Video: {video_path}")
    print(f"   Resolution: {width}x{height} | FPS: {fps} | Frames: {total_frames}")
    print("â³ Please wait, analyzing...")

    # --- Ù…ØªØºÙŠØ±Ø§Øª Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª ---
    stats = {
        "frames_processed": 0,
        "open_count": 0,
        "clasped_count": 0,
        "unknown_count": 0,
        "total_movement": 0.0,
        "movement_samples": 0
    }
    
    prev_wrists = [] # Ù„ØªØªØ¨Ø¹ Ø§Ù„Ø­Ø±ÙƒØ©

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret: break # Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ

        stats["frames_processed"] += 1
        
        # ØªØ¬Ù‡ÙŠØ² Ø§Ù„ØµÙˆØ±Ø©
        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = hands.process(image_rgb)
        
        frame_status = "Unknown" # Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø¨Ø¯Ø¦ÙŠØ© Ù„Ù‡Ø°Ø§ Ø§Ù„ÙØ±ÙŠÙ…
        current_wrists = []
        hands_data = []

        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                # 1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                landmarks = hand_landmarks.landmark
                wrist = landmarks[0]
                wrist_px = (int(wrist.x * width), int(wrist.y * height))
                current_wrists.append(wrist_px)

                # ØªØ¬Ù‡ÙŠØ² Ø§Ù„ØµÙ Ù„Ù„Ù…ÙˆØ¯ÙŠÙ„
                row = []
                middle_mcp = landmarks[9]
                hand_size = math.sqrt((wrist.x - middle_mcp.x)**2 + (wrist.y - middle_mcp.y)**2)
                scale = hand_size if hand_size > 0.01 else 1.0
                
                for lm in landmarks:
                    rel_x = (lm.x - wrist.x) / scale
                    rel_y = (lm.y - wrist.y) / scale
                    rel_z = (lm.z - wrist.z) / scale
                    row.extend([rel_x, rel_y, rel_z])
                
                # Ø§Ù„ØªÙˆÙ‚Ø¹
                pred = model.predict(np.array([row]))[0]
                hands_data.append({"wrist": wrist_px, "label": pred})

            # 2. Ù…Ù†Ø·Ù‚ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø§Ù„Ø© (Open vs Clasped)
            if len(hands_data) == 2:
                # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø³Ø§ÙØ© Ø¨ÙŠÙ† Ø§Ù„ÙŠØ¯ÙŠÙ†
                x1, y1 = hands_data[0]['wrist']
                x2, y2 = hands_data[1]['wrist']
                dist = math.sqrt((x2 - x1)**2 + (y2 - y1)**2)
                
                if dist < CLASPED_DISTANCE_THRESHOLD:
                    frame_status = "Clasped"
                else:
                    frame_status = "Open"
            
            elif len(hands_data) == 1:
                # Ù„Ùˆ ÙŠØ¯ ÙˆØ§Ø­Ø¯Ø©ØŒ Ù†Ø¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
                pred = hands_data[0]['label']
                if "Clasped" in pred: frame_status = "Clasped"
                elif "Open" in pred: frame_status = "Open"
                else: frame_status = "Open" # Ø§ÙØªØ±Ø§Ø¶ Ø£Ù† Ø§Ù„Ø¨Ø§Ù‚ÙŠ Ù…ÙØªÙˆØ­

            # 3. Ø­Ø³Ø§Ø¨ Ø³Ø±Ø¹Ø© Ø§Ù„Ø­Ø±ÙƒØ©
            frame_movement = 0
            if len(prev_wrists) == len(current_wrists) and len(current_wrists) > 0:
                for i in range(len(current_wrists)):
                    p_x, p_y = prev_wrists[i]
                    c_x, c_y = current_wrists[i]
                    move_dist = math.sqrt((c_x - p_x)**2 + (c_y - p_y)**2)
                    frame_movement += move_dist
                
                # Ù†Ø£Ø®Ø° Ø§Ù„Ù…ØªÙˆØ³Ø· Ù„Ùˆ ÙÙŠÙ‡ ÙŠØ¯ÙŠÙ†
                frame_movement /= len(current_wrists)

            if frame_movement > MOVEMENT_THRESHOLD:
                stats["total_movement"] += frame_movement
                stats["movement_samples"] += 1

            prev_wrists = current_wrists

        else:
            prev_wrists = [] # ÙÙ‚Ø¯Ù†Ø§ Ø§Ù„ØªØªØ¨Ø¹
        
        # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        if frame_status == "Clasped": stats["clasped_count"] += 1
        elif frame_status == "Open": stats["open_count"] += 1
        else: stats["unknown_count"] += 1

        # (Ø§Ø®ØªÙŠØ§Ø±ÙŠ) Ø¹Ø±Ø¶ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ - ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ù„ØºØ§Ø¤Ù‡ Ù„Ù„Ø³Ø±Ø¹Ø©
        cv2.putText(frame, f"Status: {frame_status}", (50, 50), 
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        cv2.imshow('Analyzing Video...', frame)
        if cv2.waitKey(1) & 0xFF == ord('q'): break

    cap.release()
    cv2.destroyAllWindows()
    
    # ==========================================
    # ğŸ“Š Ø¥ØµØ¯Ø§Ø± Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
    # ==========================================
    print("\n" + "="*40)
    print("       ğŸ“‹ FINAL ANALYSIS REPORT       ")
    print("="*40)
    
    total_valid_frames = stats["open_count"] + stats["clasped_count"]
    if total_valid_frames == 0: total_valid_frames = 1 # ØªØ¬Ù†Ø¨ Ø§Ù„Ù‚Ø³Ù…Ø© Ø¹Ù„Ù‰ ØµÙØ±
    
    # 1. Ø§Ù„Ù†Ø³Ø¨ Ø§Ù„Ù…Ø¦ÙˆÙŠØ©
    open_score = (stats["open_count"] / total_valid_frames) * 100
    clasped_score = (stats["clasped_count"] / total_valid_frames) * 100
    
    print(f"ğŸ”¹ Total Frames Processed: {stats['frames_processed']}")
    print(f"\nâœ‹ Hand Posture Score:")
    print(f"   âœ… Open Hand:    {open_score:.1f}%")
    print(f"   ğŸ”’ Clasped Hand: {clasped_score:.1f}%")
    
    # 2. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø±Ø¹Ø©
    avg_speed = 0
    if stats["movement_samples"] > 0:
        avg_speed = stats["total_movement"] / stats["movement_samples"]
    
    print(f"\nğŸš€ Movement Analysis:")
    print(f"   Average Speed: {avg_speed:.2f} pixels/frame")
    
    # ØªÙ‚ÙŠÙŠÙ… Ù†ØµÙŠ Ù„Ù„Ø³Ø±Ø¹Ø©
    behavior = "Calm / Stable ğŸ˜Œ"
    if avg_speed > 15: behavior = "High Energy / Nervous âš¡"
    elif avg_speed > 8: behavior = "Normal / Conversational ğŸ—£ï¸"
    
    print(f"   ğŸ“ Conclusion: The subject appears {behavior}")
    print("="*40 + "\n")

# --- ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¯Ø§Ù„Ø© ---
# Ø¶Ø¹ÙŠ Ù…Ø³Ø§Ø± Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ù‡Ù†Ø§
video_file = 'C:/Users/anesr/Downloads/interview videos/MASTER_BODY_LANGUAGE_in_JOB_INTERVIEWS_Interview_Tips_Techniques_jobinterview_720P.mp4' 
analyze_video(video_file)